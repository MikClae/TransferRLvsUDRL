{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from baselines_wrappers import DummyVecEnv, Monitor\n",
    "from pytorch_wrappers import make_atari_deepmind, BatchedPytorchFrameStack, PytorchLazyFrames\n",
    "import copy\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# init Environment\n",
    "# %pip install -U gym>=0.26.0\n",
    "# %pip install -U gym[atari,accept-rom-license]\n",
    "\n",
    "make_env = lambda: Monitor(make_atari_deepmind(\"ALE/Breakout-v5\", scale_values=True), allow_early_resets=True)\n",
    "vector_env = DummyVecEnv([make_env for _ in range(1)])\n",
    "env = BatchedPytorchFrameStack(vector_env, k=1)\n",
    "# env = gym.make('ALE/Breakout-v5')\n",
    "env.action_space.n = 4\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]\n",
    "max_reward = 90\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "horizon_scale = 0.1\n",
    "return_scale = 0.1\n",
    "replay_size = 300\n",
    "n_warm_up_episodes = 50\n",
    "n_updates_per_iter = 500\n",
    "n_episodes_per_iter = 20\n",
    "last_few = 75\n",
    "batch_size = 768\n",
    "LR = 0.0007244359600749898"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shape(net, env):\n",
    "    with torch.no_grad():\n",
    "        n_shape = net(torch.as_tensor(env.observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "    return n_shape\n",
    "\n",
    "class BF(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(BF, self).__init__()\n",
    "        self.actions = np.arange(action_space)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        depths = (48, 96, 192, 384)\n",
    "        final_layer = 256\n",
    "\n",
    "        self.fc1 = nn.Conv2d(state_space, depths[0], kernel_size=3, stride=2, padding=1)\n",
    "        self.commands = nn.Linear(2, depths[0])\n",
    "        self.fc2 = nn.ReLU()  # test\n",
    "        self.fc3 = nn.Conv2d(depths[0], depths[1], kernel_size=3, stride=2, padding=1)\n",
    "        self.fc4 = nn.ReLU()\n",
    "        self.fc5 = nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=2, padding=1)\n",
    "        self.fc6 = nn.ReLU()\n",
    "        self.fc7 = nn.Conv2d(depths[2], depths[3], kernel_size=3, stride=2, padding=1)\n",
    "        self.fc75 = nn.Flatten()\n",
    "        shape = compute_shape(nn.Sequential(self.fc1, self.fc2, self.fc3, self.fc4, self.fc5, self.fc6, self.fc7, self.fc75), env)\n",
    "        # shape = compute_shape(nn.Sequential(self.fc1, self.fc2, self.fc3, self.fc4, self.fc5, self.fc6, self.fc7), env)\n",
    "        self.fc8 = nn.Linear(shape, final_layer)\n",
    "        self.fc9 = nn.Linear(final_layer, final_layer)\n",
    "        self.fc10 = nn.Linear(final_layer, action_space)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # self.fc1 = nn.Conv2d(state_space, depths[0], kernel_size=8, stride=4)\n",
    "        # self.commands = nn.Linear(2, depths[0])\n",
    "        # self.fc2 = nn.ReLU()  # test\n",
    "        # self.fc3 = nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2)\n",
    "        # self.fc4 = nn.ReLU()\n",
    "        # self.fc5 = nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1)\n",
    "        # self.fc6 = nn.ReLU()\n",
    "        # self.fc7 = nn.Flatten()\n",
    "        # # shape = compute_shape(nn.Sequential(self.fc1, self.fc3, self.fc4, self.fc5, self.fc6, self.fc7), env)\n",
    "        # shape = compute_shape(nn.Sequential(self.fc1, self.fc2, self.fc3, self.fc4, self.fc5, self.fc6, self.fc7), env)\n",
    "        # self.fc8 = nn.Linear(shape, final_layer)\n",
    "        # self.fc9 = nn.ReLU()\n",
    "        # self.fc10 = nn.Linear(final_layer, action_space)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state, command):\n",
    "        # Original Forward function\n",
    "        # out = self.sigmoid(self.fc1(state))\n",
    "        # command_out = self.sigmoid(self.commands(command))\n",
    "        # out = out * command_out\n",
    "        # out = torch.relu(self.fc2(out))\n",
    "        # out = torch.relu(self.fc3(out))\n",
    "        # out = torch.relu(self.fc4(out))\n",
    "        # out = self.fc5(out)\n",
    "\n",
    "        out = self.fc1(state)\n",
    "        command_out = self.commands(command)\n",
    "        out = out*command_out[:, :, None, None]\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.fc75(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.fc9(out)\n",
    "        out = self.fc10(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Samples the action based on their probability\n",
    "        \"\"\"\n",
    "        command = torch.cat((desire*return_scale,horizon*horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action\n",
    "    def greedy_action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Returns the greedy action \n",
    "        \"\"\"\n",
    "        command = torch.cat((desire*return_scale,horizon*horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        action = torch.argmax(probs).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "        \n",
    "    def add_sample(self, states, actions, rewards):\n",
    "        episode = {\"states\": states, \"actions\":actions, \"rewards\": rewards, \"summed_rewards\":sum(rewards)}\n",
    "        self.buffer.append(episode)\n",
    "        \n",
    "    \n",
    "    def sort(self):\n",
    "        #sort buffer\n",
    "        self.buffer = sorted(self.buffer, key = lambda i: i[\"summed_rewards\"],reverse=True)\n",
    "        # keep the max buffer size\n",
    "        self.buffer = self.buffer[:self.max_size]\n",
    "    \n",
    "    def get_random_samples(self, batch_size):\n",
    "        self.sort()\n",
    "        idxs = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = [self.buffer[idx] for idx in idxs]\n",
    "        return batch\n",
    "    \n",
    "    def get_nbest(self, n):\n",
    "        self.sort()\n",
    "        return self.buffer[:n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init replay buffer with n-warmup runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "buffer = ReplayBuffer(replay_size)\n",
    "bf = BF(state_space, action_space).to(device)\n",
    "optimizer = optim.Adam(params=bf.parameters(), lr=LR)\n",
    "\n",
    "samples = []\n",
    "#initial command\n",
    "init_desired_reward = 1 \n",
    "init_time_horizon = 1 \n",
    "\n",
    "for i in range(n_warm_up_episodes):\n",
    "    desired_return = torch.FloatTensor([init_desired_reward])\n",
    "    desired_time_horizon = torch.FloatTensor([init_time_horizon])\n",
    "    state = env.reset()\n",
    "    state = np.stack([obs.get_frames() for obs in state])\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = bf.action(torch.from_numpy(state).float().to(device), torch.unsqueeze(desired_return, 0).to(device), torch.unsqueeze(desired_time_horizon, 0).to(device))\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state = np.stack([obs.get_frames() for obs in next_state])\n",
    "        reward = int(reward)\n",
    "        done = bool(done)\n",
    "        states.append(torch.from_numpy(state).float())\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "        \n",
    "    # if isinstance(actions[0], torch.Tensor):\n",
    "    #     for i in range(len(actions)):\n",
    "    #         actions[i] = actions[i].item()\n",
    "    buffer.add_sample(states, actions, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## OBSERVE THE WEIGHTS before training\n",
    "#for p in bf.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR Sampling exploration commands\n",
    "\n",
    "def sampling_exploration( top_X_eps = last_few):\n",
    "    \"\"\"\n",
    "    This function calculates the new desired reward and new desired horizon based on the replay buffer.\n",
    "    New desired horizon is calculted by the mean length of the best last X episodes. \n",
    "    New desired reward is sampled from a uniform distribution given the mean and the std calculated from the last best X performances.\n",
    "    where X is the hyperparameter last_few.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    top_X = buffer.get_nbest(last_few)\n",
    "    #The exploratory desired horizon dh0 is set to the mean of the lengths of the selected episodes\n",
    "    new_desired_horizon = np.mean([len(i[\"states\"]) for i in top_X])\n",
    "    # save all top_X cumulative returns in a list\n",
    "    returns = [i[\"summed_rewards\"] for i in top_X]\n",
    "    # from these returns calc the mean and std\n",
    "    mean_returns = np.mean(returns)\n",
    "    std_returns = np.std(returns)\n",
    "    # sample desired reward from a uniform distribution given the mean and the std\n",
    "    new_desired_reward = np.random.uniform(mean_returns, mean_returns+std_returns)\n",
    "\n",
    "    return torch.FloatTensor([new_desired_reward])  , torch.FloatTensor([new_desired_horizon]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR TRAINING\n",
    "def select_time_steps(saved_episode):\n",
    "    \"\"\"\n",
    "    Given a saved episode from the replay buffer this function samples random time steps (t1 and t2) in that episode:\n",
    "    T = max time horizon in that episode\n",
    "    Returns t1, t2 and T \n",
    "    \"\"\"\n",
    "    # Select times in the episode:\n",
    "    T = len(saved_episode[\"states\"]) # episode max horizon \n",
    "    t1 = np.random.randint(0,T-1)\n",
    "    t2 = np.random.randint(t1+1,T)\n",
    "\n",
    "    return t1, t2, T\n",
    "\n",
    "def create_training_input(episode, t1, t2):\n",
    "    \"\"\"\n",
    "    Based on the selected episode and the given time steps this function returns 4 values:\n",
    "    1. state at t1\n",
    "    2. the desired reward: sum over all rewards from t1 to t2\n",
    "    3. the time horizont: t2 -t1\n",
    "    \n",
    "    4. the target action taken at t1\n",
    "    \n",
    "    buffer episodes are build like [cumulative episode reward, states, actions, rewards]\n",
    "    \"\"\"\n",
    "    state = episode[\"states\"][t1] \n",
    "    desired_reward = sum(episode[\"rewards\"][t1:t2])\n",
    "    time_horizont = t2-t1\n",
    "    action = episode[\"actions\"][t1]\n",
    "    return state, desired_reward, time_horizont, action\n",
    "\n",
    "def create_training_examples(batch_size):\n",
    "    \"\"\"\n",
    "    Creates a data set of training examples that can be used to create a data loader for training.\n",
    "    ============================================================\n",
    "    1. for the given batch_size episode idx are randomly selected\n",
    "    2. based on these episodes t1 and t2 are samples for each selected episode \n",
    "    3. for the selected episode and sampled t1 and t2 trainings values are gathered\n",
    "    ______________________________________________________________\n",
    "    Output are two numpy arrays in the length of batch size:\n",
    "    Input Array for the Behavior function - consisting of (state, desired_reward, time_horizon)\n",
    "    Output Array with the taken actions \n",
    "    \"\"\"\n",
    "    state_array = []\n",
    "    desired_reward_array = []\n",
    "    time_horizon_array = []\n",
    "    output_array = []\n",
    "    # select randomly episodes from the buffer\n",
    "    episodes = buffer.get_random_samples(batch_size)\n",
    "    for ep in episodes:\n",
    "        #select time stamps\n",
    "        t1, t2, T = select_time_steps(ep)\n",
    "        # For episodic tasks they set t2 to T:\n",
    "        t2 = T\n",
    "        state, desired_reward, time_horizont, action = create_training_input(ep, t1, t2)\n",
    "        state_array.append(state)\n",
    "        desired_reward_array.append(torch.FloatTensor([desired_reward]))\n",
    "        time_horizon_array.append(torch.FloatTensor([time_horizont]))\n",
    "        output_array.append(action)\n",
    "    return state_array, desired_reward_array, time_horizon_array, output_array\n",
    "\n",
    "def train_behavior_function(batch_size):\n",
    "    \"\"\"\n",
    "    Trains the BF with on a cross entropy loss were the inputs are the action probabilities based on the state and command.\n",
    "    The targets are the actions appropriate to the states from the replay buffer.\n",
    "    \"\"\"\n",
    "    X1, X2, X3, y = create_training_examples(batch_size)\n",
    "\n",
    "    X1 = torch.cat(X1)\n",
    "    X2 = torch.stack(X2)\n",
    "    X3 = torch.stack(X3)\n",
    "    state = X1[:,0:state_space]\n",
    "    d = X2\n",
    "    h = X3\n",
    "    command = torch.cat((d*return_scale,h*horizon_scale), dim=-1)\n",
    "    y = torch.stack(y).long()\n",
    "    y_ = bf(state.to(device), command.to(device)).float()\n",
    "    optimizer.zero_grad()\n",
    "    y = torch.squeeze(y, 1)\n",
    "    pred_loss = F.cross_entropy(y_, y)\n",
    "    pred_loss.backward()\n",
    "    optimizer.step()\n",
    "    return pred_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(desired_return = torch.FloatTensor([init_desired_reward]), desired_time_horizon = torch.FloatTensor([init_time_horizon])):\n",
    "    \"\"\"\n",
    "    Runs one episode of the environment to evaluate the bf.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    rewards = 0\n",
    "    while True:\n",
    "        state = np.stack([obs.get_frames() for obs in state])\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = bf.action(state.to(device), torch.unsqueeze(desired_return, 0).to(device), torch.unsqueeze(desired_time_horizon, 0).to(device))\n",
    "        state, reward, done, _ = env.step(action.cpu().numpy()) \n",
    "        rewards += reward\n",
    "        desired_return = min(desired_return - reward, torch.FloatTensor([max_reward]))\n",
    "        desired_time_horizon = max(desired_time_horizon - 1, torch.FloatTensor([1]))\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "    return int(rewards)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2 - Generates an Episode unsing the Behavior Function:\n",
    "def generate_episode(desired_return = torch.FloatTensor([init_desired_reward]), desired_time_horizon = torch.FloatTensor([init_time_horizon])):    \n",
    "    \"\"\"\n",
    "    Generates more samples for the replay buffer.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        state = np.stack([obs.get_frames() for obs in state])\n",
    "        state = torch.FloatTensor(state)\n",
    "\n",
    "        action = bf.action(state.to(device), desired_return.to(device), desired_time_horizon.to(device))\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.unsqueeze(torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()]),0)\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "    return [states, actions, rewards]\n",
    "\n",
    "\n",
    "# Algorithm 1 - Upside - Down Reinforcement Learning \n",
    "def run_upside_down(max_episodes):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    losses = []\n",
    "    average_100_reward = []\n",
    "    desired_rewards_history = []\n",
    "    horizon_history = []\n",
    "    for ep in range(1, max_episodes+1):\n",
    "\n",
    "        # improve|optimize bf based on replay buffer\n",
    "        loss_buffer = []\n",
    "        for i in range(n_updates_per_iter):\n",
    "            bf_loss = train_behavior_function(batch_size)\n",
    "            loss_buffer.append(bf_loss)\n",
    "        bf_loss = np.mean(loss_buffer)\n",
    "        losses.append(bf_loss)\n",
    "        # run x new episode and add to buffer\n",
    "        for i in range(n_episodes_per_iter):\n",
    "\n",
    "            # Sample exploratory commands based on buffer\n",
    "            new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "            generated_episode = generate_episode(torch.unsqueeze(new_desired_reward,0), torch.unsqueeze(new_desired_horizon,0))\n",
    "            # if isinstance(generated_episode[1][0], torch.Tensor):\n",
    "            #     for i in range(len(generated_episode[1])):\n",
    "            #         generated_episode[1][i] = generated_episode[1][i].item()\n",
    "            if isinstance(generated_episode[2][0], np.ndarray):\n",
    "                for i in range(len(generated_episode[2])):\n",
    "                    generated_episode[2][i] = generated_episode[2][i][0]\n",
    "            buffer.add_sample(generated_episode[0],generated_episode[1],generated_episode[2])\n",
    "        new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "        # monitoring desired reward and desired horizon\n",
    "        desired_rewards_history.append(new_desired_reward.item())\n",
    "        horizon_history.append(new_desired_horizon.item())\n",
    "        \n",
    "        ep_rewards = evaluate(new_desired_reward, new_desired_horizon)\n",
    "        all_rewards.append(ep_rewards)\n",
    "        average_100_reward.append(np.mean(all_rewards[-100:]))\n",
    "\n",
    "        print(\"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss), end=\"\", flush=True)\n",
    "        if ep % 100 == 0:\n",
    "            print(\"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss))\n",
    "            \n",
    "    return all_rewards, average_100_reward, desired_rewards_history, horizon_history, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:1\u001B[0m\n",
      "Cell \u001B[1;32mIn[22], line 45\u001B[0m, in \u001B[0;36mrun_upside_down\u001B[1;34m(max_episodes)\u001B[0m\n\u001B[0;32m     43\u001B[0m loss_buffer \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_updates_per_iter):\n\u001B[1;32m---> 45\u001B[0m     bf_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_behavior_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     46\u001B[0m     loss_buffer\u001B[38;5;241m.\u001B[39mappend(bf_loss)\n\u001B[0;32m     47\u001B[0m bf_loss \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(loss_buffer)\n",
      "Cell \u001B[1;32mIn[20], line 77\u001B[0m, in \u001B[0;36mtrain_behavior_function\u001B[1;34m(batch_size)\u001B[0m\n\u001B[0;32m     75\u001B[0m command \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((d\u001B[38;5;241m*\u001B[39mreturn_scale,h\u001B[38;5;241m*\u001B[39mhorizon_scale), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     76\u001B[0m y \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(y)\u001B[38;5;241m.\u001B[39mlong()\n\u001B[1;32m---> 77\u001B[0m y_ \u001B[38;5;241m=\u001B[39m \u001B[43mbf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m     78\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     79\u001B[0m y \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msqueeze(y, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[15], line 63\u001B[0m, in \u001B[0;36mBF.forward\u001B[1;34m(self, state, command)\u001B[0m\n\u001B[0;32m     61\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(out)\n\u001B[0;32m     62\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc4(out)\n\u001B[1;32m---> 63\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc5\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc6(out)\n\u001B[0;32m     65\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc7(out)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rewards, average, d, h, loss = run_upside_down(max_episodes=100)\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%Hh%Mm%Ss\", t)\n",
    "save_name = \"PlotsProducedAt\" + current_time + \".png\"\n",
    "save_name_bf = \"BFProducedAt\" + current_time + \".pth\"\n",
    "torch.save(bf.state_dict(), save_name_bf)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(\"Rewards\")\n",
    "plt.plot(rewards, label=\"rewards\")\n",
    "plt.plot(average, label=\"average100\")\n",
    "plt.legend()\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss)\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(\"desired Rewards\")\n",
    "plt.plot(d)\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(\"desired Horizon\")\n",
    "plt.plot(h)\n",
    "plt.savefig(save_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# SAVE MODEL\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModelProducedAt\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[43mcurrent_time\u001B[49m \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39msave(bf\u001B[38;5;241m.\u001B[39mstate_dict(), name)\n\u001B[0;32m      4\u001B[0m executionTime \u001B[38;5;241m=\u001B[39m (time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m startTime)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'current_time' is not defined"
     ]
    }
   ],
   "source": [
    "# SAVE MODEL\n",
    "name = \"ModelProducedAt\" + current_time + \".pth\"\n",
    "torch.save(bf.state_dict(), name)\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))\n",
    "name = \"HypersProducedAt\" + current_time + \".txt\"\n",
    "with open(name, 'w') as f:\n",
    "    f.write('batch = ' + str(batch_size) + '\\n')\n",
    "    f.write('LR = ' + str(LR) + '\\n')\n",
    "    f.write('horizon_scale = ' + str(horizon_scale) + '\\n')\n",
    "    f.write('return_scale = ' + str(return_scale) + '\\n')\n",
    "    f.write('replay_size = ' + str(replay_size) + '\\n')\n",
    "    f.write('n_warm_up_episodes = ' + str(n_warm_up_episodes) + '\\n')\n",
    "    f.write('n_updates_per_iter = ' + str(n_updates_per_iter) + '\\n')\n",
    "    f.write('n_episodes_per_iter = ' + str(n_episodes_per_iter) + '\\n')\n",
    "    f.write('last_few = ' + str(last_few) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OBSERVE THE WEIGHTS after training\n",
    "#for p in bf.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESIRED_REWARD = torch.FloatTensor([200]).to(device)\n",
    "# DESIRED_HORIZON = torch.FloatTensor([200]).to(device)\n",
    "# desired = DESIRED_REWARD.item()\n",
    "#\n",
    "# env = gym.make('CartPole-v0')\n",
    "# env.reset()\n",
    "# rewards = 0\n",
    "# while True:\n",
    "#     command = torch.cat((DESIRED_REWARD*return_scale,DESIRED_HORIZON*horizon_scale), dim=-1)\n",
    "#\n",
    "#     probs_logits = bf(torch.from_numpy(state).float().to(device), command)\n",
    "#     probs = torch.softmax(probs_logits, dim=-1).detach().cpu()\n",
    "#     action = torch.argmax(probs).item()\n",
    "#     state, reward, done, info = env.step(action)\n",
    "#     rewards += reward\n",
    "#     DESIRED_REWARD -= reward\n",
    "#     DESIRED_HORIZON -= 1\n",
    "#     if done:\n",
    "#         break\n",
    "#\n",
    "# print(\"Desired rewards: {} | after finishing one episode the agent received {} rewards\".format(desired, rewards))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
