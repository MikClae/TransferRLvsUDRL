{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from baselines_wrappers import DummyVecEnv, Monitor\n",
    "from pytorch_wrappers import make_atari_deepmind, BatchedPytorchFrameStack, PytorchLazyFrames\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# init Environment\n",
    "%pip install -U gym>=0.26.0\n",
    "%pip install -U gym[atari,accept-rom-license]\n",
    "\n",
    "make_env = lambda: Monitor(make_atari_deepmind(\"ALE/Breakout-v5\", scale_values=True), allow_early_resets=True)\n",
    "vector_env = DummyVecEnv([make_env for _ in range(1)])\n",
    "env = BatchedPytorchFrameStack(vector_env, k=1)\n",
    "# env = gym.make('ALE/Breakout-v5')\n",
    "env.action_space.n = 4\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]\n",
    "max_reward = 90\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "horizon_scale = 0.02\n",
    "return_scale = 0.02\n",
    "replay_size = 700\n",
    "n_warm_up_episodes = 50\n",
    "n_updates_per_iter = 100\n",
    "n_episodes_per_iter = 15\n",
    "last_few = 50\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! 256\n",
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shape(net, env):\n",
    "    with torch.no_grad():\n",
    "        n_shape = net(torch.as_tensor(env.observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "    return n_shape\n",
    "\n",
    "class BF(nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super(BF, self).__init__()\n",
    "        self.actions = np.arange(action_space)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        depths = (32, 64, 64)\n",
    "        final_layer = 512\n",
    "\n",
    "        self.fc1 = nn.Conv2d(state_space, depths[0], kernel_size=8, stride=4)\n",
    "        self.commands = nn.Linear(2, depths[0])\n",
    "        # self.fc2 = nn.ReLU()  # test\n",
    "        self.fc3 = nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2)\n",
    "        self.fc4 = nn.ReLU()\n",
    "        self.fc5 = nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1)\n",
    "        self.fc6 = nn.ReLU()\n",
    "        self.fc7 = nn.Flatten()\n",
    "        shape = compute_shape(nn.Sequential(self.fc1, self.fc3, self.fc4, self.fc5, self.fc6, self.fc7), env)\n",
    "        # shape = compute_shape(nn.Sequential(self.fc1, self.fc2, self.fc3, self.fc4, self.fc5, self.fc6, self.fc7), env)\n",
    "        self.fc8 = nn.Linear(shape, final_layer)\n",
    "        self.fc9 = nn.ReLU()\n",
    "        self.fc10 = nn.Linear(final_layer, action_space)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, state, command):\n",
    "        # Original Forward function\n",
    "        # out = self.sigmoid(self.fc1(state))\n",
    "        # command_out = self.sigmoid(self.commands(command))\n",
    "        # out = out * command_out\n",
    "        # out = torch.relu(self.fc2(out))\n",
    "        # out = torch.relu(self.fc3(out))\n",
    "        # out = torch.relu(self.fc4(out))\n",
    "        # out = self.fc5(out)\n",
    "\n",
    "        print(\"1 state\", state)\n",
    "        print(\"1 state shape\", state.shape)\n",
    "        print(\"1 com\", command)\n",
    "        print(\"1 com shape\", command.shape)\n",
    "        out = self.sigmoid(self.fc1(state))\n",
    "        command_out = self.sigmoid(self.commands(command))\n",
    "        print(\"2 state\", out)\n",
    "        print(\"2 state shape\", out.shape)\n",
    "        print(\"2 com\", command_out)\n",
    "        print(\"2 com shape\", command_out.shape)\n",
    "        # TRY UNSQUEEZE  https://discuss.pytorch.org/t/multiplication-of-tensors-of-different-dimensions/104842\n",
    "        out = out * command_out\n",
    "        #out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.fc7(out)\n",
    "        out = self.fc8(out)\n",
    "        out = self.fc9(out)\n",
    "        out = self.fc10(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Samples the action based on their probability\n",
    "        \"\"\"\n",
    "        command = torch.cat((desire*return_scale,horizon*horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action\n",
    "    def greedy_action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Returns the greedy action \n",
    "        \"\"\"\n",
    "        command = torch.cat((desire*return_scale,horizon*horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        action = torch.argmax(probs).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "        \n",
    "    def add_sample(self, states, actions, rewards):\n",
    "        episode = {\"states\": states, \"actions\":actions, \"rewards\": rewards, \"summed_rewards\":sum(rewards)}\n",
    "        self.buffer.append(episode)\n",
    "        \n",
    "    \n",
    "    def sort(self):\n",
    "        #sort buffer\n",
    "        self.buffer = sorted(self.buffer, key = lambda i: i[\"summed_rewards\"],reverse=True)\n",
    "        # keep the max buffer size\n",
    "        self.buffer = self.buffer[:self.max_size]\n",
    "    \n",
    "    def get_random_samples(self, batch_size):\n",
    "        self.sort()\n",
    "        idxs = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = [self.buffer[idx] for idx in idxs]\n",
    "        return batch\n",
    "    \n",
    "    def get_nbest(self, n):\n",
    "        self.sort()\n",
    "        return self.buffer[:n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init replay buffer with n-warmup runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 state tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n",
      "1 state shape torch.Size([1, 1, 84, 84])\n",
      "1 com tensor([0.0200, 0.0200])\n",
      "1 com shape torch.Size([2])\n",
      "2 state tensor([[[[0.5017, 0.5017, 0.5017,  ..., 0.5101, 0.5056, 0.5017],\n",
      "          [0.5092, 0.5092, 0.5092,  ..., 0.5237, 0.4823, 0.5092],\n",
      "          [0.5117, 0.4674, 0.4610,  ..., 0.4610, 0.4595, 0.4601],\n",
      "          ...,\n",
      "          [0.5097, 0.4946, 0.4926,  ..., 0.4926, 0.4902, 0.5008],\n",
      "          [0.4799, 0.4909, 0.4926,  ..., 0.4926, 0.4903, 0.5081],\n",
      "          [0.4989, 0.4880, 0.4926,  ..., 0.4926, 0.4939, 0.5111]],\n",
      "\n",
      "         [[0.5067, 0.5067, 0.5067,  ..., 0.5338, 0.5336, 0.5067],\n",
      "          [0.5123, 0.5123, 0.5123,  ..., 0.5261, 0.5255, 0.5123],\n",
      "          [0.5411, 0.5580, 0.5577,  ..., 0.5577, 0.5594, 0.5664],\n",
      "          ...,\n",
      "          [0.5105, 0.5003, 0.4955,  ..., 0.4955, 0.4903, 0.5375],\n",
      "          [0.5227, 0.5004, 0.4955,  ..., 0.4955, 0.4878, 0.5250],\n",
      "          [0.5017, 0.4963, 0.4955,  ..., 0.4955, 0.4948, 0.5194]],\n",
      "\n",
      "         [[0.5277, 0.5277, 0.5277,  ..., 0.5364, 0.5203, 0.5277],\n",
      "          [0.5151, 0.5151, 0.5151,  ..., 0.5209, 0.5356, 0.5151],\n",
      "          [0.4948, 0.4953, 0.4931,  ..., 0.4931, 0.4935, 0.4926],\n",
      "          ...,\n",
      "          [0.5219, 0.5263, 0.5231,  ..., 0.5231, 0.5197, 0.5003],\n",
      "          [0.5206, 0.5270, 0.5231,  ..., 0.5231, 0.5181, 0.4960],\n",
      "          [0.5336, 0.5248, 0.5231,  ..., 0.5231, 0.5231, 0.5180]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5569, 0.5569, 0.5569,  ..., 0.5386, 0.5467, 0.5569],\n",
      "          [0.5596, 0.5596, 0.5596,  ..., 0.5342, 0.5808, 0.5596],\n",
      "          [0.5056, 0.4608, 0.4592,  ..., 0.4592, 0.4651, 0.4608],\n",
      "          ...,\n",
      "          [0.5371, 0.5113, 0.5080,  ..., 0.5080, 0.5162, 0.4727],\n",
      "          [0.4925, 0.5100, 0.5080,  ..., 0.5080, 0.5113, 0.4497],\n",
      "          [0.4941, 0.5107, 0.5080,  ..., 0.5080, 0.5100, 0.4964]],\n",
      "\n",
      "         [[0.4874, 0.4874, 0.4874,  ..., 0.4550, 0.5196, 0.4874],\n",
      "          [0.4794, 0.4794, 0.4794,  ..., 0.4840, 0.4793, 0.4794],\n",
      "          [0.4231, 0.4284, 0.4299,  ..., 0.4299, 0.4289, 0.4216],\n",
      "          ...,\n",
      "          [0.4744, 0.5183, 0.5123,  ..., 0.5123, 0.5176, 0.4525],\n",
      "          [0.4939, 0.5209, 0.5123,  ..., 0.5123, 0.5170, 0.4673],\n",
      "          [0.4891, 0.5144, 0.5123,  ..., 0.5123, 0.5129, 0.4939]],\n",
      "\n",
      "         [[0.5697, 0.5697, 0.5697,  ..., 0.6091, 0.5976, 0.5697],\n",
      "          [0.6234, 0.6234, 0.6234,  ..., 0.6041, 0.6488, 0.6234],\n",
      "          [0.6342, 0.5617, 0.5564,  ..., 0.5564, 0.5524, 0.5569],\n",
      "          ...,\n",
      "          [0.6607, 0.5373, 0.5295,  ..., 0.5295, 0.5204, 0.4972],\n",
      "          [0.6196, 0.5345, 0.5295,  ..., 0.5295, 0.5221, 0.4913],\n",
      "          [0.5367, 0.5324, 0.5295,  ..., 0.5295, 0.5293, 0.5160]]]],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "2 state shape torch.Size([1, 32, 20, 20])\n",
      "2 com tensor([0.4878, 0.3598, 0.4139, 0.3992, 0.3691, 0.6575, 0.6557, 0.6347, 0.3500,\n",
      "        0.3550, 0.5342, 0.4113, 0.5844, 0.5977, 0.4014, 0.6671, 0.4174, 0.4925,\n",
      "        0.5037, 0.5234, 0.3379, 0.4718, 0.5632, 0.5961, 0.6216, 0.4079, 0.3755,\n",
      "        0.5943, 0.5765, 0.4577, 0.4630, 0.4422], grad_fn=<SigmoidBackward0>)\n",
      "2 com shape torch.Size([32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[314], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m rewards \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 19\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43mbf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesired_return\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdesired_time_horizon\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     next_state, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[0;32m     21\u001B[0m     next_state \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack([obs\u001B[38;5;241m.\u001B[39mget_frames() \u001B[38;5;28;01mfor\u001B[39;00m obs \u001B[38;5;129;01min\u001B[39;00m next_state])\n",
      "Cell \u001B[1;32mIn[312], line 69\u001B[0m, in \u001B[0;36mBF.action\u001B[1;34m(self, state, desire, horizon)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;124;03mSamples the action based on their probability\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     68\u001B[0m command \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((desire\u001B[38;5;241m*\u001B[39mreturn_scale,horizon\u001B[38;5;241m*\u001B[39mhorizon_scale), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 69\u001B[0m action_prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msoftmax(action_prob, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     71\u001B[0m m \u001B[38;5;241m=\u001B[39m Categorical(probs)\n",
      "Cell \u001B[1;32mIn[312], line 51\u001B[0m, in \u001B[0;36mBF.forward\u001B[1;34m(self, state, command)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2 com\u001B[39m\u001B[38;5;124m\"\u001B[39m, command_out)\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2 com shape\u001B[39m\u001B[38;5;124m\"\u001B[39m, command_out\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 51\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcommand_out\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m#out = self.fc2(out)\u001B[39;00m\n\u001B[0;32m     53\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(out)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (20) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "\n",
    "buffer = ReplayBuffer(replay_size)\n",
    "bf = BF(state_space, action_space).to(device)\n",
    "optimizer = optim.Adam(params=bf.parameters(), lr=1e-3)\n",
    "\n",
    "samples = []\n",
    "#initial command\n",
    "init_desired_reward = 1 \n",
    "init_time_horizon = 1 \n",
    "\n",
    "for i in range(n_warm_up_episodes):\n",
    "    desired_return = torch.FloatTensor([init_desired_reward])\n",
    "    desired_time_horizon = torch.FloatTensor([init_time_horizon])\n",
    "    state = env.reset()\n",
    "    state = np.stack([obs.get_frames() for obs in state])\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = bf.action(torch.from_numpy(state).float().to(device), desired_return.to(device), desired_time_horizon.to(device))\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        next_state = np.stack([obs.get_frames() for obs in next_state])\n",
    "        reward = int(reward)\n",
    "        done = bool(done)\n",
    "        states.append(torch.from_numpy(state).float())\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "        \n",
    "    buffer.add_sample(states, actions, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## OBSERVE THE WEIGHTS before training\n",
    "#for p in bf.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR Sampling exploration commands\n",
    "\n",
    "def sampling_exploration( top_X_eps = last_few):\n",
    "    \"\"\"\n",
    "    This function calculates the new desired reward and new desired horizon based on the replay buffer.\n",
    "    New desired horizon is calculted by the mean length of the best last X episodes. \n",
    "    New desired reward is sampled from a uniform distribution given the mean and the std calculated from the last best X performances.\n",
    "    where X is the hyperparameter last_few.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    top_X = buffer.get_nbest(last_few)\n",
    "    #The exploratory desired horizon dh0 is set to the mean of the lengths of the selected episodes\n",
    "    new_desired_horizon = np.mean([len(i[\"states\"]) for i in top_X])\n",
    "    # save all top_X cumulative returns in a list \n",
    "    returns = [i[\"summed_rewards\"] for i in top_X]\n",
    "    # from these returns calc the mean and std\n",
    "    mean_returns = np.mean(returns)\n",
    "    std_returns = np.std(returns)\n",
    "    # sample desired reward from a uniform distribution given the mean and the std\n",
    "    new_desired_reward = np.random.uniform(mean_returns, mean_returns+std_returns)\n",
    "\n",
    "    return torch.FloatTensor([new_desired_reward])  , torch.FloatTensor([new_desired_horizon]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR TRAINING\n",
    "def select_time_steps(saved_episode):\n",
    "    \"\"\"\n",
    "    Given a saved episode from the replay buffer this function samples random time steps (t1 and t2) in that episode:\n",
    "    T = max time horizon in that episode\n",
    "    Returns t1, t2 and T \n",
    "    \"\"\"\n",
    "    # Select times in the episode:\n",
    "    T = len(saved_episode[\"states\"]) # episode max horizon \n",
    "    t1 = np.random.randint(0,T-1)\n",
    "    t2 = np.random.randint(t1+1,T)\n",
    "\n",
    "    return t1, t2, T\n",
    "\n",
    "def create_training_input(episode, t1, t2):\n",
    "    \"\"\"\n",
    "    Based on the selected episode and the given time steps this function returns 4 values:\n",
    "    1. state at t1\n",
    "    2. the desired reward: sum over all rewards from t1 to t2\n",
    "    3. the time horizont: t2 -t1\n",
    "    \n",
    "    4. the target action taken at t1\n",
    "    \n",
    "    buffer episodes are build like [cumulative episode reward, states, actions, rewards]\n",
    "    \"\"\"\n",
    "    state = episode[\"states\"][t1] \n",
    "    desired_reward = sum(episode[\"rewards\"][t1:t2])\n",
    "    time_horizont = t2-t1\n",
    "    action = episode[\"actions\"][t1]\n",
    "    return state, desired_reward, time_horizont, action\n",
    "\n",
    "def create_training_examples(batch_size):\n",
    "    \"\"\"\n",
    "    Creates a data set of training examples that can be used to create a data loader for training.\n",
    "    ============================================================\n",
    "    1. for the given batch_size episode idx are randomly selected\n",
    "    2. based on these episodes t1 and t2 are samples for each selected episode \n",
    "    3. for the selected episode and sampled t1 and t2 trainings values are gathered\n",
    "    ______________________________________________________________\n",
    "    Output are two numpy arrays in the length of batch size:\n",
    "    Input Array for the Behavior function - consisting of (state, desired_reward, time_horizon)\n",
    "    Output Array with the taken actions \n",
    "    \"\"\"\n",
    "    state_array = []\n",
    "    desired_reward_array = []\n",
    "    time_horizon_array = []\n",
    "    output_array = []\n",
    "    # select randomly episodes from the buffer\n",
    "    episodes = buffer.get_random_samples(batch_size)\n",
    "    for ep in episodes:\n",
    "        #select time stamps\n",
    "        t1, t2, T = select_time_steps(ep)\n",
    "        # For episodic tasks they set t2 to T:\n",
    "        t2 = T\n",
    "        state, desired_reward, time_horizont, action = create_training_input(ep, t1, t2)\n",
    "        state_array.append(state)\n",
    "        desired_reward_array.append(torch.FloatTensor([desired_reward]))\n",
    "        time_horizon_array.append(torch.FloatTensor([time_horizont]))\n",
    "        output_array.append(action)\n",
    "    return state_array, desired_reward_array, time_horizon_array, output_array\n",
    "\n",
    "def train_behavior_function(batch_size):\n",
    "    \"\"\"\n",
    "    Trains the BF with on a cross entropy loss were the inputs are the action probabilities based on the state and command.\n",
    "    The targets are the actions appropriate to the states from the replay buffer.\n",
    "    \"\"\"\n",
    "    X1, X2, X3, y = create_training_examples(batch_size)\n",
    "\n",
    "    X1 = torch.cat(X1)\n",
    "    X2 = torch.stack(X2)\n",
    "    X3 = torch.stack(X3)\n",
    "    print(X2)\n",
    "    print(X3)\n",
    "    state = X1[:,0:state_space]\n",
    "    print(\"state\", state.shape)\n",
    "    d = X2\n",
    "    print(\"d\", d.shape)\n",
    "    h = X3\n",
    "    print(\"h\", h.shape)\n",
    "    command = torch.cat((d*return_scale,h*horizon_scale), dim=-1)\n",
    "    y = torch.stack(y).long()\n",
    "    print(\"state-command\", state, command)\n",
    "    y_ = bf(state.to(device), command.to(device)).float()\n",
    "    optimizer.zero_grad()\n",
    "    pred_loss = F.cross_entropy(y_, y)   \n",
    "    pred_loss.backward()\n",
    "    optimizer.step()\n",
    "    return pred_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(desired_return = torch.FloatTensor([init_desired_reward]), desired_time_horizon = torch.FloatTensor([init_time_horizon])):\n",
    "    \"\"\"\n",
    "    Runs one episode of the environment to evaluate the bf.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    rewards = 0\n",
    "    while True:\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = bf.action(state.to(device), desired_return.to(device), desired_time_horizon.to(device))\n",
    "        state, reward, done, _ = env.step(action.cpu().numpy()) \n",
    "        rewards += reward\n",
    "        desired_return = min(desired_return - reward, torch.FloatTensor([max_reward]))\n",
    "        desired_time_horizon = max(desired_time_horizon - 1, torch.FloatTensor([1]))\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "    return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2 - Generates an Episode unsing the Behavior Function:\n",
    "def generate_episode(desired_return = torch.FloatTensor([init_desired_reward]), desired_time_horizon = torch.FloatTensor([init_time_horizon])):    \n",
    "    \"\"\"\n",
    "    Generates more samples for the replay buffer.\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        state = torch.FloatTensor(state)\n",
    "\n",
    "        action = bf.action(state.to(device), desired_return.to(device), desired_time_horizon.to(device))\n",
    "        next_state, reward, done, _ = env.step(action.cpu().numpy())\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "    return [states, actions, rewards]\n",
    "\n",
    "\n",
    "# Algorithm 1 - Upside - Down Reinforcement Learning \n",
    "def run_upside_down(max_episodes):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    losses = []\n",
    "    average_100_reward = []\n",
    "    desired_rewards_history = []\n",
    "    horizon_history = []\n",
    "    for ep in range(1, max_episodes+1):\n",
    "\n",
    "        # improve|optimize bf based on replay buffer\n",
    "        loss_buffer = []\n",
    "        for i in range(n_updates_per_iter):\n",
    "            bf_loss = train_behavior_function(batch_size)\n",
    "            loss_buffer.append(bf_loss)\n",
    "        bf_loss = np.mean(loss_buffer)\n",
    "        losses.append(bf_loss)\n",
    "        \n",
    "        # run x new episode and add to buffer\n",
    "        for i in range(n_episodes_per_iter):\n",
    "            \n",
    "            # Sample exploratory commands based on buffer\n",
    "            new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "            generated_episode = generate_episode(new_desired_reward, new_desired_horizon)\n",
    "            buffer.add_sample(generated_episode[0],generated_episode[1],generated_episode[2])\n",
    "            \n",
    "        new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "        # monitoring desired reward and desired horizon\n",
    "        desired_rewards_history.append(new_desired_reward.item())\n",
    "        horizon_history.append(new_desired_horizon.item())\n",
    "        \n",
    "        ep_rewards = evaluate(new_desired_reward, new_desired_horizon)\n",
    "        all_rewards.append(ep_rewards)\n",
    "        average_100_reward.append(np.mean(all_rewards[-100:]))\n",
    "        \n",
    "\n",
    "\n",
    "        print(\"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss), end=\"\", flush=True)\n",
    "        if ep % 100 == 0:\n",
    "            print(\"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss))\n",
    "            \n",
    "    return all_rewards, average_100_reward, desired_rewards_history, horizon_history, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rewards, average, d, h, loss = run_upside_down(max_episodes=200)\n",
    "torch.save(bf.state_dict(), \"behaviorfunction.pth\")\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(\"Rewards\")\n",
    "plt.plot(rewards, label=\"rewards\")\n",
    "plt.plot(average, label=\"average100\")\n",
    "plt.legend()\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss)\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(\"desired Rewards\")\n",
    "plt.plot(d)\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(\"desired Horizon\")\n",
    "plt.plot(h)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "name = \"model.pth\"\n",
    "torch.save(bf.state_dict(), name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OBSERVE THE WEIGHTS after training\n",
    "#for p in bf.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_REWARD = torch.FloatTensor([200]).to(device)\n",
    "DESIRED_HORIZON = torch.FloatTensor([200]).to(device)\n",
    "desired = DESIRED_REWARD.item()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "rewards = 0\n",
    "while True:\n",
    "    command = torch.cat((DESIRED_REWARD*return_scale,DESIRED_HORIZON*horizon_scale), dim=-1)\n",
    "\n",
    "    probs_logits = bf(torch.from_numpy(state).float().to(device), command)\n",
    "    probs = torch.softmax(probs_logits, dim=-1).detach().cpu()\n",
    "    action = torch.argmax(probs).item()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards += reward\n",
    "    DESIRED_REWARD -= reward\n",
    "    DESIRED_HORIZON -= 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Desired rewards: {} | after finishing one episode the agent received {} rewards\".format(desired, rewards))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
