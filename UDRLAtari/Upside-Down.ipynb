{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ale_py'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[121], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01male_py\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ALEInterface\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m     12\u001B[0m startTime \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'ale_py'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from ale_py import ALEInterface\n",
    "import time\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init Environment\n",
    "\n",
    "ale = ALEInterface()\n",
    "#print(gym.envs.registration.registry.keys())\n",
    "env = gym.make('ALE/Breakout-v5')\n",
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.shape[0]\n",
    "max_reward = 90\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "horizon_scale = 0.02\n",
    "return_scale = 0.02\n",
    "replay_size = 700\n",
    "n_warm_up_episodes = 50\n",
    "n_updates_per_iter = 100\n",
    "n_episodes_per_iter = 15\n",
    "last_few = 50\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavior Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BF(nn.Module):\n",
    "    def __init__(self, state_space, action_space, hidden_size, seed):\n",
    "        super(BF, self).__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.actions = np.arange(action_space)\n",
    "        self.action_space = action_space\n",
    "        self.fc1 = nn.Linear(state_space, hidden_size)\n",
    "        self.commands = nn.Linear(2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc5 = nn.Linear(hidden_size, action_space)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, state, command):       \n",
    "               \n",
    "        out = self.sigmoid(self.fc1(state))\n",
    "        command_out = self.sigmoid(self.commands(command))\n",
    "        out = out * command_out\n",
    "        out = torch.relu(self.fc2(out))\n",
    "        out = torch.relu(self.fc3(out))\n",
    "        out = torch.relu(self.fc4(out))\n",
    "        out = self.fc5(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Samples the action based on their probability\n",
    "        \"\"\"\n",
    "        command = torch.cat((desire*return_scale,horizon*horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action\n",
    "    def greedy_action(self, state, desire, horizon):\n",
    "        \"\"\"\n",
    "        Returns the greedy action \n",
    "        \"\"\"\n",
    "        command = torch.cat((desire*return_scale,horizon*horizon_scale), dim=-1)\n",
    "        action_prob = self.forward(state, command)\n",
    "        probs = torch.softmax(action_prob, dim=-1)\n",
    "        action = torch.argmax(probs).item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "        \n",
    "        \n",
    "    def add_sample(self, states, actions, rewards):\n",
    "        episode = {\"states\": states, \"actions\":actions, \"rewards\": rewards, \"summed_rewards\":sum(rewards)}\n",
    "        self.buffer.append(episode)\n",
    "        \n",
    "    \n",
    "    def sort(self):\n",
    "        #sort buffer\n",
    "        self.buffer = sorted(self.buffer, key = lambda i: i[\"summed_rewards\"],reverse=True)\n",
    "        # keep the max buffer size\n",
    "        self.buffer = self.buffer[:self.max_size]\n",
    "    \n",
    "    def get_random_samples(self, batch_size):\n",
    "        self.sort()\n",
    "        idxs = np.random.randint(0, len(self.buffer), batch_size)\n",
    "        batch = [self.buffer[idx] for idx in idxs]\n",
    "        return batch\n",
    "    \n",
    "    def get_nbest(self, n):\n",
    "        self.sort()\n",
    "        return self.buffer[:n]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### init replay buffer with n-warmup runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "buffer = ReplayBuffer(replay_size)\n",
    "bf = BF(state_space, action_space, 64, 1).to(device)\n",
    "optimizer = optim.Adam(params=bf.parameters(), lr=1e-3)\n",
    "\n",
    "samples = []\n",
    "#initial command\n",
    "init_desired_reward = 1 \n",
    "init_time_horizon = 1 \n",
    "\n",
    "for i in range(n_warm_up_episodes):\n",
    "    desired_return = torch.FloatTensor([init_desired_reward])\n",
    "    desired_time_horizon = torch.FloatTensor([init_time_horizon])\n",
    "    state = env.reset()[0]\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        action = bf.action(torch.from_numpy(state).float().to(device), desired_return.to(device), desired_time_horizon.to(device))\n",
    "        next_state, reward, done, done1, info = env.step(action.cpu().numpy())\n",
    "        states.append(torch.from_numpy(state).float())\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])\n",
    "\n",
    "        if done:\n",
    "            break \n",
    "        \n",
    "    buffer.add_sample(states, actions, rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## OBSERVE THE WEIGHTS before training\n",
    "#for p in bf.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR Sampling exploration commands\n",
    "\n",
    "def sampling_exploration( top_X_eps = last_few):\n",
    "    \"\"\"\n",
    "    This function calculates the new desired reward and new desired horizon based on the replay buffer.\n",
    "    New desired horizon is calculted by the mean length of the best last X episodes. \n",
    "    New desired reward is sampled from a uniform distribution given the mean and the std calculated from the last best X performances.\n",
    "    where X is the hyperparameter last_few.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    top_X = buffer.get_nbest(last_few)\n",
    "    #The exploratory desired horizon dh0 is set to the mean of the lengths of the selected episodes\n",
    "    new_desired_horizon = np.mean([len(i[\"states\"]) for i in top_X])\n",
    "    # save all top_X cumulative returns in a list \n",
    "    returns = [i[\"summed_rewards\"] for i in top_X]\n",
    "    # from these returns calc the mean and std\n",
    "    mean_returns = np.mean(returns)\n",
    "    std_returns = np.std(returns)\n",
    "    # sample desired reward from a uniform distribution given the mean and the std\n",
    "    new_desired_reward = np.random.uniform(mean_returns, mean_returns+std_returns)\n",
    "\n",
    "    return torch.FloatTensor([new_desired_reward])  , torch.FloatTensor([new_desired_horizon]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS FOR TRAINING\n",
    "def select_time_steps(saved_episode):\n",
    "    \"\"\"\n",
    "    Given a saved episode from the replay buffer this function samples random time steps (t1 and t2) in that episode:\n",
    "    T = max time horizon in that episode\n",
    "    Returns t1, t2 and T \n",
    "    \"\"\"\n",
    "    # Select times in the episode:\n",
    "    T = len(saved_episode[\"states\"]) # episode max horizon \n",
    "    t1 = np.random.randint(0,T-1)\n",
    "    t2 = np.random.randint(t1+1,T)\n",
    "\n",
    "    return t1, t2, T\n",
    "\n",
    "def create_training_input(episode, t1, t2):\n",
    "    \"\"\"\n",
    "    Based on the selected episode and the given time steps this function returns 4 values:\n",
    "    1. state at t1\n",
    "    2. the desired reward: sum over all rewards from t1 to t2\n",
    "    3. the time horizont: t2 -t1\n",
    "    \n",
    "    4. the target action taken at t1\n",
    "    \n",
    "    buffer episodes are build like [cumulative episode reward, states, actions, rewards]\n",
    "    \"\"\"\n",
    "    state = episode[\"states\"][t1] \n",
    "    desired_reward = sum(episode[\"rewards\"][t1:t2])\n",
    "    time_horizont = t2-t1\n",
    "    action = episode[\"actions\"][t1]\n",
    "    return state, desired_reward, time_horizont, action\n",
    "\n",
    "def create_training_examples(batch_size):\n",
    "    \"\"\"\n",
    "    Creates a data set of training examples that can be used to create a data loader for training.\n",
    "    ============================================================\n",
    "    1. for the given batch_size episode idx are randomly selected\n",
    "    2. based on these episodes t1 and t2 are samples for each selected episode \n",
    "    3. for the selected episode and sampled t1 and t2 trainings values are gathered\n",
    "    ______________________________________________________________\n",
    "    Output are two numpy arrays in the length of batch size:\n",
    "    Input Array for the Behavior function - consisting of (state, desired_reward, time_horizon)\n",
    "    Output Array with the taken actions \n",
    "    \"\"\"\n",
    "    input_array = []\n",
    "    output_array = []\n",
    "    # select randomly episodes from the buffer\n",
    "    episodes = buffer.get_random_samples(batch_size)\n",
    "    for ep in episodes:\n",
    "        #select time stamps\n",
    "        t1, t2, T = select_time_steps(ep)\n",
    "        # For episodic tasks they set t2 to T:\n",
    "        t2 = T\n",
    "        state, desired_reward, time_horizont, action = create_training_input(ep, t1, t2)\n",
    "        input_array.append(torch.cat([state, torch.FloatTensor([desired_reward]), torch.FloatTensor([time_horizont])]))\n",
    "        output_array.append(action)\n",
    "    return input_array, output_array\n",
    "\n",
    "def train_behavior_function(batch_size):\n",
    "    \"\"\"\n",
    "    Trains the BF with on a cross entropy loss were the inputs are the action probabilities based on the state and command.\n",
    "    The targets are the actions appropriate to the states from the replay buffer.\n",
    "    \"\"\"\n",
    "    X, y = create_training_examples(batch_size)\n",
    "\n",
    "    X = torch.stack(X)\n",
    "    state = X[:,0:state_space]\n",
    "    d = X[:,state_space:state_space+1]\n",
    "    h = X[:,state_space+1:state_space+2]\n",
    "    command = torch.cat((d*return_scale,h*horizon_scale), dim=-1)\n",
    "    y = torch.stack(y).long()\n",
    "    y_ = bf(state.to(device), command.to(device)).float()\n",
    "    optimizer.zero_grad()\n",
    "    pred_loss = F.cross_entropy(y_, y)   \n",
    "    pred_loss.backward()\n",
    "    optimizer.step()\n",
    "    return pred_loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(desired_return = torch.FloatTensor([init_desired_reward]), desired_time_horizon = torch.FloatTensor([init_time_horizon])):\n",
    "    \"\"\"\n",
    "    Runs one episode of the environment to evaluate the bf.\n",
    "    \"\"\"\n",
    "    state = env.reset()[0]\n",
    "    rewards = 0\n",
    "    while True:\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = bf.action(state.to(device), desired_return.to(device), desired_time_horizon.to(device))\n",
    "        state, reward, done, done1, _ = env.step(action.cpu().numpy())\n",
    "        rewards += reward\n",
    "        desired_return = min(desired_return - reward, torch.FloatTensor([max_reward]))\n",
    "        desired_time_horizon = max(desired_time_horizon - 1, torch.FloatTensor([1]))\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "    return rewards\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2 - Generates an Episode unsing the Behavior Function:\n",
    "def generate_episode(desired_return = torch.FloatTensor([init_desired_reward]), desired_time_horizon = torch.FloatTensor([init_time_horizon])):    \n",
    "    \"\"\"\n",
    "    Generates more samples for the replay buffer.\n",
    "    \"\"\"\n",
    "    state = env.reset()[0]\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "        state = torch.FloatTensor(state)\n",
    "\n",
    "        action = bf.action(state.to(device), desired_return.to(device), desired_time_horizon.to(device))\n",
    "        next_state, reward, done, done1, _ = env.step(action.cpu().numpy())\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        desired_return -= reward\n",
    "        desired_time_horizon -= 1\n",
    "        desired_time_horizon = torch.FloatTensor([np.maximum(desired_time_horizon, 1).item()])\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "    return [states, actions, rewards]\n",
    "\n",
    "\n",
    "# Algorithm 1 - Upside - Down Reinforcement Learning \n",
    "def run_upside_down(max_episodes):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    all_rewards = []\n",
    "    losses = []\n",
    "    average_100_reward = []\n",
    "    desired_rewards_history = []\n",
    "    horizon_history = []\n",
    "    for ep in range(1, max_episodes+1):\n",
    "\n",
    "        # improve|optimize bf based on replay buffer\n",
    "        loss_buffer = []\n",
    "        for i in range(n_updates_per_iter):\n",
    "            bf_loss = train_behavior_function(batch_size)\n",
    "            loss_buffer.append(bf_loss)\n",
    "        bf_loss = np.mean(loss_buffer)\n",
    "        losses.append(bf_loss)\n",
    "        \n",
    "        # run x new episode and add to buffer\n",
    "        for i in range(n_episodes_per_iter):\n",
    "            \n",
    "            # Sample exploratory commands based on buffer\n",
    "            new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "            generated_episode = generate_episode(new_desired_reward, new_desired_horizon)\n",
    "            buffer.add_sample(generated_episode[0],generated_episode[1],generated_episode[2])\n",
    "            \n",
    "        new_desired_reward, new_desired_horizon = sampling_exploration()\n",
    "        # monitoring desired reward and desired horizon\n",
    "        desired_rewards_history.append(new_desired_reward.item())\n",
    "        horizon_history.append(new_desired_horizon.item())\n",
    "        \n",
    "        ep_rewards = evaluate(new_desired_reward, new_desired_horizon)\n",
    "        all_rewards.append(ep_rewards)\n",
    "        average_100_reward.append(np.mean(all_rewards[-100:]))\n",
    "        \n",
    "\n",
    "\n",
    "        print(\"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss), end=\"\", flush=True)\n",
    "        if ep % 100 == 0:\n",
    "            print(\"\\rEpisode: {} | Rewards: {:.2f} | Mean_100_Rewards: {:.2f} | Loss: {:.2f}\".format(ep, ep_rewards, np.mean(all_rewards[-100:]), bf_loss))\n",
    "            \n",
    "    return all_rewards, average_100_reward, desired_rewards_history, horizon_history, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rewards, average, d, h, loss = run_upside_down(max_episodes=200)\n",
    "torch.save(bf.state_dict(), \"behaviorfunction.pth\")\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.title(\"Rewards\")\n",
    "plt.plot(rewards, label=\"rewards\")\n",
    "plt.plot(average, label=\"average100\")\n",
    "plt.legend()\n",
    "plt.subplot(2,2,2)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss)\n",
    "plt.subplot(2,2,3)\n",
    "plt.title(\"desired Rewards\")\n",
    "plt.plot(d)\n",
    "plt.subplot(2,2,4)\n",
    "plt.title(\"desired Horizon\")\n",
    "plt.plot(h)\n",
    "plt.show()\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%Hh%Mm%Ss\", t)\n",
    "save_name = \"PlotsProducedAt\" + current_time\n",
    "plt.savefig(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "name = \"model.pth\"\n",
    "torch.save(bf.state_dict(), name)\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OBSERVE THE WEIGHTS after training\n",
    "#for p in bf.parameters():\n",
    "#    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIRED_REWARD = torch.FloatTensor([200]).to(device)\n",
    "DESIRED_HORIZON = torch.FloatTensor([200]).to(device)\n",
    "desired = DESIRED_REWARD.item()\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "rewards = 0\n",
    "while True:\n",
    "    command = torch.cat((DESIRED_REWARD*return_scale,DESIRED_HORIZON*horizon_scale), dim=-1)\n",
    "\n",
    "    probs_logits = bf(torch.from_numpy(state).float().to(device), command)\n",
    "    probs = torch.softmax(probs_logits, dim=-1).detach().cpu()\n",
    "    action = torch.argmax(probs).item()\n",
    "    state, reward, done, done1, info = env.step(action)\n",
    "    rewards += reward\n",
    "    DESIRED_REWARD -= reward\n",
    "    DESIRED_HORIZON -= 1\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(\"Desired rewards: {} | after finishing one episode the agent received {} rewards\".format(desired, rewards))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
